{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load hourly resampled & merged df\n",
    "#import pandas as pd\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://docs.google.com/uc?export=download&id=1--eG9u-siOXAwVvB_P_t4gxkzYcwILMa', 'merge_export41_meteoLC102_hourly.csv')\n",
    "#df_hourly = pd.read_csv('merge_export41_meteoLC102_hourly.csv', index_col = 0)\n",
    "#df_hourly.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinguishing between true missing values and no-event 'observations'\n",
    "The time series seems to contain many missing values for noise event variables. Not all of these are true missing values, since not registering an event may simply mean there is no event. In an effort to distinguish between true missing values and no-event 'observations', we first determine the missing time periods. Considering only the gaps > 1 day, we note the following:\n",
    "\n",
    "for most locations, gaps are nearly always >= 6 days, the few exceptions have a length of 1-2 days. These exceptions may still (largely) consist of true no-event 'observations'\n",
    "\n",
    "-> (arbitrary) cutoff at 2 days?\n",
    "\n",
    "MP08bis - Vrijthof is the anomaly with 55 missing periods (vs max 5 for other locations), half of which are < 3 days, 75% of which are < 6 days. This is likely the result of its location vs that of the other locations (courtyard of the Town Hall vs along the Naamsestraat).\n",
    "\n",
    "-> (very arbitrary) cutoff at 7 days?\n",
    "\n",
    "On another note: Interestingly, most of the (supported) events registered at Vrijthof are classified as human singing. This may be linked to the nearby presence of Het Radiohuis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "def add_end(ts):\n",
    "  upper = pd.DataFrame(ts.iloc[0:1,:].replace([0,1], np.nan))   \n",
    "  upper['begin_date'] = pd.to_datetime('2023-01-01 00:00:00')\n",
    "  return pd.concat([ts, upper])\n",
    "\n",
    "def missing_or_noevent(ts, cutoff = 2, cutoff_Vrijthof = 7):\n",
    "  '''\n",
    "  This function fills some of the nans in the noise event variables with zeroes, based on the specified cutoff values. \n",
    "  The argument 'cutoff_Vrijthof' is used to specify the maximum length of a time period with missing values (in days!)\n",
    "  before it is considered truly missing for MP08bis, 'cutoff' does the same for all other MPs.\n",
    "  It returns two DataFrames: the adapted input DataFrame, and a DataFrame with all the missing time periods. \n",
    "  The latter is not filtered by the specified cutoffs. \n",
    "  'Unsupported' category is assumed to consist of unclassifiable events and is thus treated as an event, not missing.\n",
    "  '''\n",
    "\n",
    "  # Get time series sampling frequency\n",
    "  ts['timestamp'] = pd.to_datetime(ts['timestamp'])\n",
    "  ts = ts.sort_values(['location_csv','timestamp']).reset_index(drop=True)\n",
    "  freq = pd.to_timedelta(ts.loc[1, 'timestamp'] - ts.loc[0, 'timestamp'])\n",
    "\n",
    "  # Construct a df with the missing time periods\n",
    "  missing_time = pd.DataFrame()\n",
    "  df = ts.dropna(subset = 'human_noise')\n",
    "  missing_time[['location_csv', 'begin_date']] = df[['location_csv', 'timestamp']]\n",
    "  missing_time = missing_time.groupby('location_csv').apply(add_end).reset_index(drop=True)\n",
    "  missing_time = missing_time.sort_values([\"location_csv\",\"begin_date\"]).reset_index(drop=True)\n",
    "  missing_time['end_date'] = missing_time['begin_date'].shift(-1)\n",
    "  missing_time['begin_date'] = missing_time['begin_date'] + freq\n",
    "  missing_time['timedelta'] = missing_time.groupby(\"location_csv\")[\"begin_date\"].diff().shift(-1)\n",
    "  missing_time = missing_time.dropna()\n",
    "  missing_time = missing_time[missing_time['timedelta'] > freq]\n",
    "  missing_time['timedelta'] = missing_time['timedelta'] - freq\n",
    "\n",
    "  # Filter by cutoff values\n",
    "  true_na_vh = missing_time.loc[(missing_time['timedelta'] > timedelta(days = cutoff_Vrijthof)) & (missing_time['location_csv'] == '280324_mp08bis---vrijthof.csv')]\n",
    "  true_na_other = missing_time.loc[(missing_time['timedelta'] > timedelta(days = cutoff)) & (missing_time['location_csv'] != '280324_mp08bis---vrijthof.csv')]\n",
    "\n",
    "\n",
    "  col_to_fill = ts.columns[ts.columns.str.contains('noise')].values.tolist()\n",
    "\n",
    "  ts_vh = ts.loc[ts['location_csv'] == '280324_mp08bis---vrijthof.csv' ].copy()\n",
    "  ts_other = ts.loc[ts['location_csv'] != '280324_mp08bis---vrijthof.csv'].copy()\n",
    "\n",
    "  \n",
    "  # Add column true_na yes/no (less intervals to check for true nans than false nans)\n",
    "  # If timestamp not in true_na, replace any nans with 0s\n",
    "\n",
    "    #Vrijthof\n",
    "  ts_vh['true_na'] = ts_vh['timestamp'].apply(lambda t: any((true_na_vh[\"begin_date\"] <= t) & (true_na_vh[\"end_date\"] > t)))\n",
    "  ts_vh.loc[ts_vh['true_na'] == 0, col_to_fill] = ts_vh.loc[ts_vh['true_na'] == 0, col_to_fill].fillna(0)\n",
    "\n",
    "    #other MPs\n",
    "  other = ts_other['location_csv'].drop_duplicates().tolist()\n",
    "  for MP in other:\n",
    "    ts_other.loc[ts_other['location_csv'] == MP,'true_na'] = ts_other.loc[ts_other['location_csv'] == MP, 'timestamp'] \\\n",
    "                          .apply(lambda t: any((true_na_other['location_csv'] == MP) &(true_na_other[\"begin_date\"] <= t) & (true_na_other[\"end_date\"] > t)))\n",
    "  ts_other.loc[ts_other['true_na'] == 0, col_to_fill] = ts_other.loc[ts_other['true_na'] == 0, col_to_fill].fillna(0)\n",
    "\n",
    "  df = pd.concat([ts_vh, ts_other]).drop('true_na', axis = 1)  \n",
    "\n",
    "  return df, missing_time\n",
    "\n",
    "df, missing = missing_or_noevent(df_hourly)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature building\n",
    "Could still be added: window features, such as 'number of time periods with noise event, out of the last 6 time periods' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "from datetime import timedelta\n",
    "\n",
    "# DEV: \n",
    "## GPT prompt:\n",
    "## if I have a pandas dataframe with timestamps (UTC), can you suggest some additional features to engineer? (for example: weekend, weekday, Belgian holiday, season, ...)\n",
    "## Can you provide python3 code on an imaginary dataframe?\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "# Add weekday/weekend feature\n",
    "df[\"is_weekend\"] = df[\"timestamp\"].dt.weekday.isin([5, 6]).astype(int)\n",
    "\n",
    "# Add day of the week feature\n",
    "df[\"day_of_week\"] = df[\"timestamp\"].dt.day_name()\n",
    "\n",
    "# Add hour of the day feature\n",
    "df[\"hour_of_day\"] = df[\"timestamp\"].dt.hour\n",
    "\n",
    "# Add time of day feature\n",
    "df[\"time_of_day\"] = pd.cut(\n",
    "    df[\"timestamp\"].dt.hour,\n",
    "    bins=[-1, 6, 12, 18, 24],\n",
    "    labels=[\"Night\", \"Morning\", \"Afternoon\", \"Evening\"],\n",
    ")  # equal bins\n",
    "\n",
    "# Add season feature\n",
    "df[\"season\"] = pd.cut(\n",
    "    df[\"timestamp\"].dt.month,\n",
    "    bins=[0, 3, 6, 9, 12],\n",
    "    labels=[\"Winter\", \"Spring\", \"Summer\", \"Fall\"],\n",
    ")\n",
    "\n",
    "# Add month feature\n",
    "df[\"month\"] = df[\"timestamp\"].dt.month_name()\n",
    "\n",
    "# Add day of the month feature\n",
    "df[\"day_of_month\"] = df[\"timestamp\"].dt.day\n",
    "\n",
    "# Add quarter feature\n",
    "df[\"quarter\"] = \"Q\" + df[\"timestamp\"].dt.quarter.astype(str)\n",
    "\n",
    "# Add Belgian holidays feature\n",
    "be_holidays = holidays.BE()\n",
    "df[\"is_be_holiday\"] = (\n",
    "    df[\"timestamp\"].dt.date.astype(\"datetime64\").isin(be_holidays).astype(int)\n",
    ")\n",
    "\n",
    "# Add business day feature\n",
    "df[\"is_business_day\"] = ~df[\"timestamp\"].dt.weekday.isin([5, 6]) & ~df[\"is_be_holiday\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature indicating whether the response variable is missing\n",
    "df['missing'] = df['human_noise'].isna()\n",
    "\n",
    "# Create dataframe with exam & vacation dates based on the academic calendars 2021-2022 & 2022-2023 of the KU Leuven & KU Leuven Group T \n",
    "# https://www.kuleuven.be/over-kuleuven/kalenders/kalenders-21-22 & https://www.kuleuven.be/over-kuleuven/kalenders \n",
    "# vacation = periods with no lessons or exams lasting at least 1 week\n",
    "kul_ac_year = pd.DataFrame({\"begin_date\": [\"2022-01-10\", \"2022-05-30\", \"2022-01-10\", \"2022-05-30\",\n",
    "                                           \"2022-01-31\", \"2022-06-27\", \"2022-01-01\", \"2022-02-05\",\n",
    "                                           \"2022-04-02\", \"2022-07-02\", \"2022-12-24\"],\n",
    "                    \"end_date\": [\"2022-02-04\", \"2022-07-01\", \"2022-01-30\", \"2022-06-26\", \n",
    "                                 \"2022-02-04\", \"2022-07-01\", \"2022-01-13\", \"2022-02-13\", \n",
    "                                 \"2022-04-18\", \"2022-09-25\", \"2022-12-31\"],\n",
    "                    \"type\": [\"exams\", \"exams\", \"first_exam_weeks\", \"first_exam_weeks\", \n",
    "                             \"final_exam_week\", \"final_exam_week\", \"vacation\", \"vacation\", \n",
    "                             \"vacation\", \"vacation\", \"vacation\"]})\n",
    "\n",
    "kul_ac_year[\"begin_date\"] = pd.to_datetime(kul_ac_year[\"begin_date\"])\n",
    "kul_ac_year[\"end_date\"] = pd.to_datetime(kul_ac_year[\"end_date\"])\n",
    "kul_ac_year[\"end_date\"] = kul_ac_year[\"end_date\"] + pd.Timedelta('23:59:59')\n",
    "\n",
    "kul_ac_year\n",
    "\n",
    "# Add (university) exams feature (only first & second exam period)\n",
    "df[\"exams\"] = df[\"timestamp\"].apply(lambda t: any((kul_ac_year[\"type\"] == \"exams\") & (kul_ac_year[\"begin_date\"] <= t) & (kul_ac_year[\"end_date\"] >= t)))\n",
    "\n",
    "  # Alternatively add 2 features, one for first, 'normal', exam weeks, one for the last week\n",
    "df[\"first_exam_weeks\"] = df[\"timestamp\"].apply(lambda t: any((kul_ac_year[\"type\"] == \"first_exam_weeks\") & (kul_ac_year[\"begin_date\"] <= t) & (kul_ac_year[\"end_date\"] >= t)))\n",
    "df[\"final_exam_week\"] = df[\"timestamp\"].apply(lambda t: any((kul_ac_year[\"type\"] == \"final_exam_week\") & (kul_ac_year[\"begin_date\"] <= t) & (kul_ac_year[\"end_date\"] >= t)))\n",
    "\n",
    "# Add student vacation periods feature\n",
    "df[\"student_vacation\"] = df[\"timestamp\"].apply(lambda t: any((kul_ac_year[\"type\"] == \"vacation\") & (kul_ac_year[\"begin_date\"] <= t) & (kul_ac_year[\"end_date\"] >= t)))\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting with skforecast and LightGBM\n",
    "Code used as blueprint:\n",
    "Forecasting time series with gradient boosting: Skforecast, XGBoost, LightGBM y CatBoost by JoaquÃ­n Amat Rodrigo and Javier Escobar Ortiz, available under a Attribution 4.0 International (CC BY 4.0) at https://www.cienciadedatos.net/documentos/py39-forecasting-time-series-with-skforecast-xgboost-lightgbm-catboost.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "from skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate\n",
    "from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\n",
    "from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\n",
    "from skforecast.utils import save_forecaster, load_forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create input dfs\n",
    "# Drop location 08bis due to no human noise events occuring during validation month (November)\n",
    "df = df[df['location_csv'] != '280324_mp08bis---vrijthof.csv']\n",
    "\n",
    "# Create separate df with target time series \n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "ts_data = pd.pivot_table(data = df, values = 'human_noise', index = 'timestamp', columns = 'location_csv')\n",
    "\n",
    "# Additional category for missing response -> multiclass for some locations\n",
    "ts_data = ts_data.fillna(2)\n",
    "ts_data = ts_data.astype('category')\n",
    "\n",
    "# Drop last row (not real data) \n",
    "ts_data.drop(index=ts_data.index[-1],axis=0,inplace=True)\n",
    "\n",
    "# Explicitly set hourly frequency of datetime index\n",
    "ts_data.index.freq = 'H'\n",
    "\n",
    "# Create dictionary with exogenous data for every location \n",
    "# necessary because missing values are location-specific\n",
    "locations = df['location_csv'].drop_duplicates().tolist()\n",
    "exog_datasets = {}\n",
    "\n",
    "for location in locations:\n",
    "  # Subset by location\n",
    "  somename = df.loc[df['location_csv'] == location,:]\n",
    "\n",
    "  # Create df with exogenous vars\n",
    "  cat_exogs = ['missing',\n",
    "              'is_weekend',\n",
    "              'day_of_week',\n",
    "              'time_of_day',\n",
    "              'is_be_holiday',\n",
    "              'is_business_day',\n",
    "              'season',\n",
    "              'month',\n",
    "              'first_exam_weeks', \n",
    "              'final_exam_week',\n",
    "              'student_vacation']\n",
    "  num_exogs = ['hour_of_day',\n",
    "              'LC_TEMP_QCL3', \n",
    "              'LC_WINDSPEED', \n",
    "              'LC_RAININ']\n",
    "\n",
    "  exog_data  = somename[cat_exogs + num_exogs + ['timestamp']].set_index('timestamp').sort_index()\n",
    "  exog_data[cat_exogs] = exog_data[cat_exogs].astype('category')\n",
    "\n",
    "  # Drop last row (not real data) \n",
    "  exog_data.drop(index=exog_data.index[-1],axis=0,inplace=True)\n",
    "\n",
    "  # Explicitly set hourly frequency of datetime index\n",
    "  exog_data.index.freq = 'H'\n",
    "\n",
    "  # Add to dictionary\n",
    "  exog_datasets[location] = exog_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependent multivariate time series analysis with direct multi-step forecasting\n",
    "\n",
    "# Defining train-val-test-split dates,\n",
    "# starting from the time the last sensor began registering events of any type\n",
    "# to minimize missing values in response var and lag predictors\n",
    "start_train = '2022-03-07 22:00:00' \n",
    "end_train = '2022-10-31 23:59:00'\n",
    "end_val = '2022-11-30 23:59:00'\n",
    "\n",
    "# Different class imbalance per location -> different weights\n",
    "# -> create custom weights function for every location \n",
    "# (skforecast only allows a function based on datetime index, not simply a column)\n",
    "def createFunc(location):\n",
    "  ts_train = ts_data.loc[start_train:end_train, location]\n",
    "  ts = ts_data.loc[start_train:, location]\n",
    "  minority_weight = ts_train.value_counts()[0] / ts_train.value_counts()[1]\n",
    "  weights = np.where(ts == 1, minority_weight, 1)\n",
    "\n",
    "  def custom_weights(index):\n",
    "    w = weights[:len(index)]\n",
    "    return w\n",
    "\n",
    "  return custom_weights\n",
    "\n",
    "# Create dictionary of weight functions\n",
    "weight_funcs = [createFunc(location) for location in locations]\n",
    "weight_funcs = {k:v for k,v in zip(locations, weight_funcs)}\n",
    "\n",
    "# Define metric to calculate f1-score for positive label regardless of whether there's an additional 'missing' class\n",
    "def f1score_pos(y_true, y_pred):\n",
    "  scores = f1_score(y_true, y_pred, average = None)\n",
    "  return scores[1]\n",
    "\n",
    "# Transform categorical features using ordinal encoding. \n",
    "# Numeric features are left untouched. \n",
    "# if a new category is found in the test set, it is encoded as -1.\n",
    "\n",
    "pipeline_cat = make_pipeline(OrdinalEncoder(dtype=int, #int dtype required by skforecast \n",
    "                                            handle_unknown=\"use_encoded_value\",\n",
    "                                            unknown_value=-1,),\n",
    "                             FunctionTransformer(func=lambda x: x.astype('category'), \n",
    "                                                 feature_names_out = 'one-to-one'))\n",
    "transformer_exog = make_column_transformer((pipeline_cat, cat_exogs),\n",
    "                                           remainder=\"passthrough\").set_output(transform=\"pandas\")\n",
    "\n",
    "# Define max lags\n",
    "lags = 48\n",
    "\n",
    "# Define forecast horizon\n",
    "h = 12\n",
    "\n",
    "# Define grids\n",
    "lags_grid = {12, 24, 36, 48}\n",
    "param_grid = {'min_child_samples': [10, 20, 30],\n",
    "              'num_leaves': [5, 10, 15, 20, 25]}\n",
    "\n",
    "# Gridsearch for all locations\n",
    "LGBM_gs = {}\n",
    "for level in locations:\n",
    "  \n",
    "  # Initializing the forecaster\n",
    "  forecaster = ForecasterAutoregMultiVariate(regressor = LGBMClassifier(categorical_features='auto', random_state=123, max_depth = 6),\n",
    "                                            lags = lags,\n",
    "                                            level = level,\n",
    "                                            steps = h, \n",
    "                                            transformer_exog = transformer_exog,\n",
    "                                            weight_func = weight_funcs[level]\n",
    "  )\n",
    "\n",
    "  # Grid search\n",
    "  result = grid_search_forecaster_multiseries(forecaster = forecaster,\n",
    "                                                series = ts_data.loc[start_train:end_val, :],\n",
    "                                                exog = exog_datasets[level].loc[start_train:end_val],\n",
    "                                                lags_grid = lags_grid,\n",
    "                                                param_grid = param_grid,\n",
    "                                                steps = h,\n",
    "                                                metric = f1score_pos,\n",
    "                                                initial_train_size = len(ts_data.loc[start_train:end_train]),\n",
    "                                                refit = False,\n",
    "                                                fixed_train_size = False,\n",
    "                                                return_best = False,\n",
    "                                                verbose = False\n",
    "  )      \n",
    "  #print(result.to_string())\n",
    "  LGBM_gs[level] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in locations:\n",
    "  gs = LGBM_gs[level]\n",
    "  best = gs.sort_values('f1score_pos', ascending = False).reset_index().loc[0, 'f1score_pos']\n",
    "  print(level, best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model & fitting it on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining train-val-test-split dates,\n",
    "# starting from the time the last sensor began registering events of any type\n",
    "# to minimize missing values in response var and lag predictors\n",
    "start_train = '2022-03-07 22:00:00' \n",
    "end_train = '2022-10-31 23:59:00'\n",
    "end_val = '2022-11-30 23:59:00'\n",
    "\n",
    "# Define metric to calculate f1-score for every label separately\n",
    "def f1score(y_true, y_pred):\n",
    "  scores = f1_score(y_true, y_pred, average = None)\n",
    "  return scores\n",
    "\n",
    "# Define function to change column type to categorical, since lambda function cannot be saved by built-in method\n",
    "def to_cat(x):\n",
    "  x = x.astype('category')\n",
    "  return x\n",
    "\n",
    "# Transform categorical features using ordinal encoding. \n",
    "# Numeric features are left untouched. \n",
    "# if a new category is found in the test set, it is encoded as -1.\n",
    "pipeline_cat = make_pipeline(OrdinalEncoder(dtype=int, # int dtype required by skforecast \n",
    "                                            handle_unknown=\"use_encoded_value\",\n",
    "                                           unknown_value=-1,),\n",
    "                             FunctionTransformer(func = to_cat, \n",
    "                                                 feature_names_out = 'one-to-one'))\n",
    "transformer_exog = make_column_transformer((pipeline_cat, cat_exogs),\n",
    "                                           remainder=\"passthrough\").set_output(transform=\"pandas\")\n",
    "\n",
    "# Define forecast horizon\n",
    "h = 12 \n",
    "\n",
    "# Testing best model for each location \n",
    "# And fitting each of those on all data\n",
    "metrics_best = pd.DataFrame()\n",
    "predictions_best = {}\n",
    "forecasters = {}\n",
    "\n",
    "for level in locations:\n",
    "  gs = LGBM_gs[level]\n",
    "  min_child_samples = gs.sort_values('f1score_pos', ascending = False).reset_index().loc[0, 'min_child_samples']\n",
    "  num_leaves = gs.sort_values('f1score_pos', ascending = False).reset_index().loc[0, 'num_leaves']\n",
    "  lags = gs.sort_values('f1score_pos', ascending = False).reset_index().loc[0, 'lags']\n",
    "\n",
    "  # Custom_weights function moved inside loop since nested function cannot be saved\n",
    "  def get_custom_weights(index):\n",
    "    ts_train = ts_data.loc[start_train:end_val, level]\n",
    "    ts = ts_data.loc[start_train:, level]\n",
    "    minority_weight = ts_train.value_counts()[0] / ts_train.value_counts()[1]\n",
    "    weights = np.where(ts == 1, minority_weight, 1)\n",
    "    w = weights[:len(index)]\n",
    "    return w\n",
    "\n",
    "  # Initializing the forecaster\n",
    "  forecaster = ForecasterAutoregMultiVariate(regressor = LGBMClassifier(categorical_features='auto', \n",
    "                                                                        random_state=123,\n",
    "                                                                        min_child_samples = min_child_samples,\n",
    "                                                                        num_leaves = num_leaves,\n",
    "                                                                        max_depth = 6\n",
    "                                                                        ),\n",
    "                                            lags = lags,\n",
    "                                            level = level,\n",
    "                                            steps = h, \n",
    "                                            transformer_exog = transformer_exog,\n",
    "                                            weight_func = get_custom_weights\n",
    "  )\n",
    "  \n",
    "  # Testing best model for each location \n",
    "  metric_best, preds_best = backtesting_forecaster_multiseries(forecaster = forecaster,\n",
    "                                                series = ts_data.loc[start_train:],\n",
    "                                                exog = exog_datasets[level].loc[start_train:],\n",
    "                                                steps = h,\n",
    "                                                metric = f1score,\n",
    "                                                initial_train_size = len(ts_data.loc[start_train:end_val]),\n",
    "                                                refit = False,\n",
    "                                                fixed_train_size = False,\n",
    "                                                verbose = False\n",
    "  )\n",
    "  metrics_best = pd.concat([metrics_best, metric_best])\n",
    "  predictions_best[level] = preds_best \n",
    "\n",
    "  # Training best model for each location on all data\n",
    "  forecaster.fit(series = ts_data.loc[start_train:],\n",
    "                 exog = exog_datasets[level].loc[start_train:])    \n",
    "\n",
    "  forecasters[level] = forecaster \n",
    "  name = level[:-4]\n",
    "  save_forecaster(forecaster, file_name = f'/forecasters/forecaster_h12_{name}.pkl')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_best\n",
    "# many f1-scores for positive label are 0 due to either no true postives, \n",
    "# or no predicted positives (undefined f1-score defaults to 0)\n",
    "# If missing values are a label, its f1-score = 1, \n",
    "# indicating that adding the missing-value variable worked as intended"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to load forecasters & use them to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all forecasters\n",
    "loaded_forecasters = {}\n",
    "for level in locations:\n",
    "  name = level[:-4]\n",
    "  loaded_forecaster = load_forecaster(file_name = f'/forecasters/forecaster_h12_{name}.pkl', verbose = False)\n",
    "  loaded_forecasters[level] = loaded_forecaster\n",
    "\n",
    "# Create dataframe with all of the exogeneous variables that were used to train model\n",
    "# values for weather can/should be made adaptable through sliders \n",
    "# 99% sure that you can use the same exog dataframe for all locations\n",
    "# here I just take a part of a training exog_data and overwrite its timestamps, obviously don't do that in the final implementation\n",
    "exog = exog_datasets['255443_mp-06-parkstraat-2-la-filosovia.csv'][500:512].reset_index().set_index(pd.date_range(start = '2023.01.01 00:00:00', periods = 12, freq = 'H'))\n",
    "\n",
    "# Generate predictions for the next 12 hours for one location\n",
    "loaded_forecasters['255439_mp-01-naamsestraat-35-maxim.csv'].predict(steps = 12, exog = exog)\n",
    "\n",
    "# Generate prediction for the 7th hour\n",
    "loaded_forecasters['255439_mp-01-naamsestraat-35-maxim.csv'].predict(steps = [7], exog = exog)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
