{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load hourly resampled & merged df\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://docs.google.com/uc?export=download&id=1--eG9u-siOXAwVvB_P_t4gxkzYcwILMa', 'merge_export41_meteoLC102_hourly.csv')\n",
    "df_hourly = pd.read_csv('merge_export41_meteoLC102_hourly.csv', index_col = 0)\n",
    "df_hourly.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinguishing between true missing values and no-event 'observations'\n",
    "The time series seems to contain many missing values for noise event variables. Not all of these are true missing values, since not registering an event may simply mean there is no event. In an effort to distinguish between true missing values and no-event 'observations', we first determine the missing time periods. Considering only the gaps > 1 day, we note the following:\n",
    "\n",
    "for most locations, gaps are nearly always >= 6 days, the few exceptions have a length of 1-2 days. These exceptions may still (largely) consist of true no-event 'observations'\n",
    "\n",
    "-> (arbitrary) cutoff at 2 days?\n",
    "\n",
    "MP08bis - Vrijthof is the anomaly with 55 missing periods (vs max 5 for other locations), half of which are < 3 days, 75% of which are < 6 days. This is likely the result of its location vs that of the other locations (courtyard of the Town Hall vs along the Naamsestraat).\n",
    "\n",
    "-> (very arbitrary) cutoff at 7 days?\n",
    "\n",
    "On another note: Interestingly, most of the (supported) events registered at Vrijthof are classified as human singing. This may be linked to the nearby presence of Het Radiohuis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "def add_end(ts):\n",
    "  upper = pd.DataFrame(ts.iloc[0:1,:].replace([0,1], np.nan))   \n",
    "  upper['begin_date'] = pd.to_datetime('2023-01-01 00:00:00')\n",
    "  return pd.concat([ts, upper])\n",
    "\n",
    "def missing_or_noevent(ts, cutoff = 2, cutoff_Vrijthof = 7):\n",
    "  '''\n",
    "  This function fills some of the nans in the noise event variables with zeroes, based on the specified cutoff values. \n",
    "  The argument 'cutoff_Vrijthof' is used to specify the maximum length of a time period with missing values (in days!)\n",
    "  before it is considered truly missing for MP08bis, 'cutoff' does the same for all other MPs.\n",
    "  It returns two DataFrames: the adapted input DataFrame, and a DataFrame with all the missing time periods. \n",
    "  The latter is not filtered by the specified cutoffs. \n",
    "  'Unsupported' category is assumed to consist of unclassifiable events and is thus treated as an event, not missing.\n",
    "  '''\n",
    "\n",
    "  # Get time series sampling frequency\n",
    "  ts['timestamp'] = pd.to_datetime(ts['timestamp'])\n",
    "  ts = ts.sort_values(['location_csv','timestamp']).reset_index(drop=True)\n",
    "  freq = pd.to_timedelta(ts.loc[1, 'timestamp'] - ts.loc[0, 'timestamp'])\n",
    "\n",
    "  # Construct a df with the missing time periods\n",
    "  missing_time = pd.DataFrame()\n",
    "  df = ts.dropna(subset = 'human_noise')\n",
    "  missing_time[['location_csv', 'begin_date']] = df[['location_csv', 'timestamp']]\n",
    "  missing_time = missing_time.groupby('location_csv').apply(add_end).reset_index(drop=True)\n",
    "  missing_time = missing_time.sort_values([\"location_csv\",\"begin_date\"]).reset_index(drop=True)\n",
    "  missing_time['end_date'] = missing_time['begin_date'].shift(-1)\n",
    "  missing_time['begin_date'] = missing_time['begin_date'] + freq\n",
    "  missing_time['timedelta'] = missing_time.groupby(\"location_csv\")[\"begin_date\"].diff().shift(-1)\n",
    "  missing_time = missing_time.dropna()\n",
    "  missing_time = missing_time[missing_time['timedelta'] > freq]\n",
    "  missing_time['timedelta'] = missing_time['timedelta'] - freq\n",
    "\n",
    "  # Filter by cutoff values\n",
    "  true_na_vh = missing_time.loc[(missing_time['timedelta'] > timedelta(days = cutoff_Vrijthof)) & (missing_time['location_csv'] == '280324_mp08bis---vrijthof.csv')]\n",
    "  true_na_other = missing_time.loc[(missing_time['timedelta'] > timedelta(days = cutoff)) & (missing_time['location_csv'] != '280324_mp08bis---vrijthof.csv')]\n",
    "\n",
    "\n",
    "  col_to_fill = ts.columns[ts.columns.str.contains('noise')].values.tolist()\n",
    "\n",
    "  ts_vh = ts.loc[ts['location_csv'] == '280324_mp08bis---vrijthof.csv' ].copy()\n",
    "  ts_other = ts.loc[ts['location_csv'] != '280324_mp08bis---vrijthof.csv'].copy()\n",
    "\n",
    "  \n",
    "  # Add column true_na yes/no (less intervals to check for true nans than false nans)\n",
    "  # If timestamp not in true_na, replace any nans with 0s\n",
    "\n",
    "    #Vrijthof\n",
    "  ts_vh['true_na'] = ts_vh['timestamp'].apply(lambda t: any((true_na_vh[\"begin_date\"] <= t) & (true_na_vh[\"end_date\"] > t)))\n",
    "  ts_vh.loc[ts_vh['true_na'] == 0, col_to_fill] = ts_vh.loc[ts_vh['true_na'] == 0, col_to_fill].fillna(0)\n",
    "\n",
    "    #other MPs\n",
    "  other = ts_other['location_csv'].drop_duplicates().tolist()\n",
    "  for MP in other:\n",
    "    ts_other.loc[ts_other['location_csv'] == MP,'true_na'] = ts_other.loc[ts_other['location_csv'] == MP, 'timestamp'] \\\n",
    "                          .apply(lambda t: any((true_na_other['location_csv'] == MP) &(true_na_other[\"begin_date\"] <= t) & (true_na_other[\"end_date\"] > t)))\n",
    "  ts_other.loc[ts_other['true_na'] == 0, col_to_fill] = ts_other.loc[ts_other['true_na'] == 0, col_to_fill].fillna(0)\n",
    "\n",
    "  df = pd.concat([ts_vh, ts_other]).drop('true_na', axis = 1)  \n",
    "\n",
    "  return df, missing_time\n",
    "\n",
    "df, missing = missing_or_noevent(df_hourly)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature building\n",
    "Could still be added: window features, such as 'number of time periods with noise event, out of the last 6 time periods' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "from datetime import timedelta\n",
    "\n",
    "# DEV: \n",
    "## GPT prompt:\n",
    "## if I have a pandas dataframe with timestamps (UTC), can you suggest some additional features to engineer? (for example: weekend, weekday, Belgian holiday, season, ...)\n",
    "## Can you provide python3 code on an imaginary dataframe?\n",
    "\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "# Add weekday/weekend feature\n",
    "df[\"is_weekend\"] = df[\"timestamp\"].dt.weekday.isin([5, 6]).astype(int)\n",
    "\n",
    "# Add day of the week feature\n",
    "df[\"day_of_week\"] = df[\"timestamp\"].dt.day_name()\n",
    "\n",
    "# Add hour of the day feature\n",
    "df[\"hour_of_day\"] = df[\"timestamp\"].dt.hour\n",
    "\n",
    "# Add time of day feature\n",
    "df[\"time_of_day\"] = pd.cut(\n",
    "    df[\"timestamp\"].dt.hour,\n",
    "    bins=[-1, 6, 12, 18, 24],\n",
    "    labels=[\"Night\", \"Morning\", \"Afternoon\", \"Evening\"],\n",
    ")  # equal bins\n",
    "\n",
    "# Add season feature\n",
    "df[\"season\"] = pd.cut(\n",
    "    df[\"timestamp\"].dt.month,\n",
    "    bins=[0, 3, 6, 9, 12],\n",
    "    labels=[\"Winter\", \"Spring\", \"Summer\", \"Fall\"],\n",
    ")\n",
    "\n",
    "# Add month feature\n",
    "df[\"month\"] = df[\"timestamp\"].dt.month_name()\n",
    "\n",
    "# Add day of the month feature\n",
    "df[\"day_of_month\"] = df[\"timestamp\"].dt.day\n",
    "\n",
    "# Add quarter feature\n",
    "df[\"quarter\"] = \"Q\" + df[\"timestamp\"].dt.quarter.astype(str)\n",
    "\n",
    "# Add Belgian holidays feature\n",
    "be_holidays = holidays.BE()\n",
    "df[\"is_be_holiday\"] = (\n",
    "    df[\"timestamp\"].dt.date.astype(\"datetime64\").isin(be_holidays).astype(int)\n",
    ")\n",
    "\n",
    "# Add business day feature\n",
    "df[\"is_business_day\"] = ~df[\"timestamp\"].dt.weekday.isin([5, 6]) & ~df[\"is_be_holiday\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature indicating whether the response variable is missing\n",
    "df['missing'] = df['human_noise'].isna()\n",
    "\n",
    "# Create dataframe with exam & vacation dates based on the academic calendars 2021-2022 & 2022-2023 of the KU Leuven & KU Leuven Group T \n",
    "# https://www.kuleuven.be/over-kuleuven/kalenders/kalenders-21-22 & https://www.kuleuven.be/over-kuleuven/kalenders \n",
    "# vacation = periods with no lessons or exams lasting at least 1 week\n",
    "kul_ac_year = pd.DataFrame({\"begin_date\": [\"2022-01-10\", \"2022-05-30\", \"2022-01-10\", \"2022-05-30\",\n",
    "                                           \"2022-01-31\", \"2022-06-27\", \"2022-01-01\", \"2022-02-05\",\n",
    "                                           \"2022-04-02\", \"2022-07-02\", \"2022-12-24\"],\n",
    "                    \"end_date\": [\"2022-02-04\", \"2022-07-01\", \"2022-01-30\", \"2022-06-26\", \n",
    "                                 \"2022-02-04\", \"2022-07-01\", \"2022-01-13\", \"2022-02-13\", \n",
    "                                 \"2022-04-18\", \"2022-09-25\", \"2022-12-31\"],\n",
    "                    \"type\": [\"exams\", \"exams\", \"first_exam_weeks\", \"first_exam_weeks\", \n",
    "                             \"final_exam_week\", \"final_exam_week\", \"vacation\", \"vacation\", \n",
    "                             \"vacation\", \"vacation\", \"vacation\"]})\n",
    "\n",
    "kul_ac_year[\"begin_date\"] = pd.to_datetime(kul_ac_year[\"begin_date\"])\n",
    "kul_ac_year[\"end_date\"] = pd.to_datetime(kul_ac_year[\"end_date\"])\n",
    "kul_ac_year[\"end_date\"] = kul_ac_year[\"end_date\"] + pd.Timedelta('23:59:59')\n",
    "\n",
    "kul_ac_year\n",
    "\n",
    "# Add (university) exams feature (only first & second exam period)\n",
    "df[\"exams\"] = df[\"timestamp\"].apply(lambda t: any((kul_ac_year[\"type\"] == \"exams\") & (kul_ac_year[\"begin_date\"] <= t) & (kul_ac_year[\"end_date\"] >= t)))\n",
    "\n",
    "  # Alternatively add 2 features, one for first, 'normal', exam weeks, one for the last week\n",
    "df[\"first_exam_weeks\"] = df[\"timestamp\"].apply(lambda t: any((kul_ac_year[\"type\"] == \"first_exam_weeks\") & (kul_ac_year[\"begin_date\"] <= t) & (kul_ac_year[\"end_date\"] >= t)))\n",
    "df[\"final_exam_week\"] = df[\"timestamp\"].apply(lambda t: any((kul_ac_year[\"type\"] == \"final_exam_week\") & (kul_ac_year[\"begin_date\"] <= t) & (kul_ac_year[\"end_date\"] >= t)))\n",
    "\n",
    "# Add student vacation periods feature\n",
    "df[\"student_vacation\"] = df[\"timestamp\"].apply(lambda t: any((kul_ac_year[\"type\"] == \"vacation\") & (kul_ac_year[\"begin_date\"] <= t) & (kul_ac_year[\"end_date\"] >= t)))\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting with skforecast and LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "\n",
    "from skforecast.ForecasterAutoregMultiVariate import ForecasterAutoregMultiVariate\n",
    "from skforecast.model_selection_multiseries import backtesting_forecaster_multiseries\n",
    "from skforecast.model_selection_multiseries import grid_search_forecaster_multiseries\n",
    "from skforecast.utils import save_forecaster, load_forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create input dfs\n",
    "# Drop location 08bis due to no human noise events occuring during validation month (November)\n",
    "df = df[df['location_csv'] != '280324_mp08bis---vrijthof.csv']\n",
    "\n",
    "# Create separate df with target time series \n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "ts_data = pd.pivot_table(data = df, values = 'human_noise', index = 'timestamp', columns = 'location_csv')\n",
    "\n",
    "# Additional category for missing response -> multiclass for some locations\n",
    "ts_data = ts_data.fillna(2)\n",
    "ts_data = ts_data.astype('category')\n",
    "\n",
    "# Drop last row (not real data) \n",
    "ts_data.drop(index=ts_data.index[-1],axis=0,inplace=True)\n",
    "\n",
    "# Explicitly set hourly frequency of datetime index\n",
    "ts_data.index.freq = 'H'\n",
    "\n",
    "# Create dictionary with exogenous data for every location \n",
    "# necessary because missing values are location-specific\n",
    "locations = df['location_csv'].drop_duplicates().tolist()\n",
    "exog_datasets = {}\n",
    "\n",
    "for location in locations:\n",
    "  # Subset by location\n",
    "  somename = df.loc[df['location_csv'] == location,:]\n",
    "\n",
    "  # Create df with exogenous vars\n",
    "  cat_exogs = ['missing',\n",
    "              'is_weekend',\n",
    "              'day_of_week',\n",
    "              'time_of_day',\n",
    "              'is_be_holiday',\n",
    "              'is_business_day',\n",
    "              'season',\n",
    "              'month',\n",
    "              'first_exam_weeks', \n",
    "              'final_exam_week',\n",
    "              'student_vacation']\n",
    "  num_exogs = ['hour_of_day',\n",
    "              'LC_TEMP_QCL3', \n",
    "              'LC_WINDSPEED', \n",
    "              'LC_RAININ']\n",
    "\n",
    "  exog_data  = somename[cat_exogs + num_exogs + ['timestamp']].set_index('timestamp').sort_index()\n",
    "  exog_data[cat_exogs] = exog_data[cat_exogs].astype('category')\n",
    "\n",
    "  # Drop last row (not real data) \n",
    "  exog_data.drop(index=exog_data.index[-1],axis=0,inplace=True)\n",
    "\n",
    "  # Explicitly set hourly frequency of datetime index\n",
    "  exog_data.index.freq = 'H'\n",
    "\n",
    "  # Add to dictionary\n",
    "  exog_datasets[location] = exog_data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
